{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:43:55.428391Z",
     "start_time": "2023-05-24T18:43:55.420480Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, pipeline, PreTrainedTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "log.addHandler(handler)\n",
    "\n",
    "tqdm.pandas()\n",
    "# some tokenizers require this\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:44:04.931679Z",
     "start_time": "2023-05-24T18:44:02.246446Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"medmcqa\", split=\"train\").to_pandas()\n",
    "data.fillna(\"\", inplace=True)\n",
    "data[\"text\"] = data[\"question\"] + \"\\n\" + data[\"exp\"]\n",
    "data = data[data[\"subject_name\"] == \"Medicine\"]\n",
    "data = data.sample(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to check:\n",
    "- `jarvisx17/medicine-ner` - doesn't work\n",
    "- `ukkendane/bert-medical-ner` - works quite well\n",
    "- `samrawal/bert-large-uncased_med-ner` - too heavy and results are questionable\n",
    "- `samrawal/bert-base-uncased_clinical-ner` - better than previous one, but still some broken ents (fixed with \"first\" strategy?)\n",
    "- `reginaboateng/clinical_bert_adapter_ner_pico_for_classification_task` - adapter-transformers lib is needed, so let's skip it but it's probably ok model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:44:07.839283Z",
     "start_time": "2023-05-24T18:44:07.826628Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    model_name: str, data: pd.DataFrame, aggregation_strategy=None\n",
    ") -> pd.DataFrame:\n",
    "    pipe = pipeline(\n",
    "        task=\"ner\", model=model_name, aggregation_strategy=aggregation_strategy\n",
    "    )\n",
    "    data[\"res\"] = data[\"text\"].progress_map(pipe)\n",
    "    print(len(data))\n",
    "    # at least a single entity is predicted\n",
    "    data_with_res = data[data[\"res\"].map(bool)]\n",
    "    log.info(\"Data len: {0}\".format(len(data)))\n",
    "    log.info(\"Number or records with entities: {0}\".format(len(data_with_res)))\n",
    "    return data_with_res\n",
    "\n",
    "\n",
    "def print_some_results(data_with_res: pd.DataFrame, sample: Union[int, float] = 5):\n",
    "    for i, row in data_with_res.sample(min(sample, len(data_with_res))).iterrows():\n",
    "        print(\"Id:\", i, \"\\n\")\n",
    "        print(\"Input text:\", row[\"text\"], \"\\b\")\n",
    "        print(\"Entities:\")\n",
    "        pprint(row[\"res\"])\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "class EntityDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        tag_sep: str = \"-\",\n",
    "        entity_key: str = \"entity\",\n",
    "    ):\n",
    "        \"\"\"Replacement for hf ner pipeline if it doesn't work\n",
    "\n",
    "        Use to assemble human-readable entities from classified tokens.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (transformers.AutoTokenizer): proper transformers tokenizer\n",
    "            tag_sep (str, optional): separator in class names for bio tags\n",
    "                (in B-PERSON \"-\" is a separator). Defaults to \"-\".\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag_sep = tag_sep\n",
    "        self.entity_key = entity_key\n",
    "\n",
    "    def __call__(self, ents: list) -> list:\n",
    "        \"\"\"Transform list of token entities from NER model to words\n",
    "\n",
    "        Args:\n",
    "            ents (list): list of dicts with entities\n",
    "\n",
    "        Returns:\n",
    "            list: list or assembled entities into words\n",
    "        \"\"\"\n",
    "        grouped_ents = self._group(ents)\n",
    "        return self._merge(grouped_ents)\n",
    "\n",
    "    def _group(self, ents):\n",
    "        if not ents:\n",
    "            return []\n",
    "        res_ents = []\n",
    "\n",
    "        # TODO: test if it's better to just use another if in the main loop\n",
    "        for i, ent in enumerate(ents):\n",
    "            if ent[self.entity_key].startswith(f\"B{self.tag_sep}\"):\n",
    "                current_entity = [ent]\n",
    "                current_entity_class = current_entity[0][self.entity_key].split(\n",
    "                    self.tag_sep, maxsplit=1\n",
    "                )[1]\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "        for ent in ents[i + 1 :]:\n",
    "            if ent[self.entity_key].startswith(f\"B{self.tag_sep}\"):\n",
    "                res_ents.append(current_entity)\n",
    "                current_entity = [ent]\n",
    "                current_entity_class = current_entity[0][self.entity_key].split(\n",
    "                    self.tag_sep, maxsplit=1\n",
    "                )[1]\n",
    "            elif (\n",
    "                ent[self.entity_key] == f\"I{self.tag_sep}{current_entity_class}\"\n",
    "                and ent[\"start\"] - 1 == current_entity[-1][\"end\"]\n",
    "            ):\n",
    "                current_entity.append(ent)\n",
    "            else:\n",
    "                pass\n",
    "                # skip entities that start with I tag\n",
    "        res_ents.append(current_entity)\n",
    "        return res_ents\n",
    "\n",
    "    def _merge(self, grouped_ents):\n",
    "        res = []\n",
    "        for ent in grouped_ents:\n",
    "            temp_ent = {\n",
    "                \"tag\": ent[0][self.entity_key].split(self.tag_sep, maxsplit=1)[1],\n",
    "                \"text\": self.tokenizer.convert_tokens_to_string(\n",
    "                    [x[\"word\"] for x in ent]\n",
    "                ),\n",
    "                \"start\": ent[0][\"start\"],\n",
    "                \"end\": ent[-1][\"end\"],\n",
    "            }\n",
    "            res.append(temp_ent)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:44:10.677453Z",
     "start_time": "2023-05-24T18:44:08.434946Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"jarvisx17/medicine-ner\"\n",
    "\n",
    "test_jarvisx17 = test_model(model_name, data)\n",
    "\n",
    "print_some_results(test_jarvisx17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:24:14.875498Z",
     "start_time": "2023-05-24T18:24:14.394711Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"ukkendane/bert-medical-ner\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ent_decoder_underscore = EntityDecoder(tokenizer=tokenizer, entity_key=\"entity_group\", tag_sep=\"_\")\n",
    "\n",
    "test_ukkendane = test_model(model_name, data[:100], aggregation_strategy=\"first\")\n",
    "\n",
    "test_ukkendane['res'] = test_ukkendane['res'].map(ent_decoder_underscore)\n",
    "\n",
    "# print_some_results(test_ukkendane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"samrawal/bert-base-uncased_clinical-ner\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ent_decoder_dash = EntityDecoder(tokenizer=tokenizer, tag_sep=\"-\")\n",
    "\n",
    "test_samrawal_base = test_model(model_name, data, aggregation_strategy=\"first\")\n",
    "\n",
    "print_some_results(test_samrawal_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:24:14.877421Z",
     "start_time": "2023-05-24T18:24:14.877407Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"samrawal/bert-large-uncased_med-ner\"\n",
    "\n",
    "test_samrawal = test_model(model_name, data, aggregation_strategy=\"first\")\n",
    "\n",
    "print_some_results(test_samrawal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skills",
   "language": "python",
   "name": "skills"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
